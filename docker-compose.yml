services:
  # FastAPI 主服务
  jumo-api:
    build: .
    image: jumo-service:latest
    container_name: jumo-api
    restart: unless-stopped
    ports:
      # 宿主 5116 -> 容器 5116 (FastAPI)
      # 这是整个体系中唯一需要暴露的端口
      - "${API_SERVICE_PORT:-5116}:${API_SERVICE_PORT:-5116}"   
    environment:
      - API_SERVICE_PORT=${API_SERVICE_PORT:-5116}
      - PYTHON_PATH=${PYTHON_PATH}
      - PYTHONPATH=/app/src
      - REDIS_HOST=redis # Redis 配置（通过docker-compose内的服务名redis访问）
      - REDIS_PORT=${REDIS_PORT:-6379}
      - REDIS_DB=${REDIS_DB:-0} 
      - REDIS_PASSWORD=${REDIS_PASSWORD:-}
      - WORKER_QUEUE_NAME=${WORKER_QUEUE_NAME:-celery}
      - TASK_NAME_PROCESS_PDF=${TASK_NAME_PROCESS_PDF:-process_pdf}
      - CELERY_REDIS_DB_BROKER=${CELERY_REDIS_DB_BROKER:-0}
      - CELERY_REDIS_DB_BACKEND=${CELERY_REDIS_DB_BACKEND:-1}

      # MinIO 配置
      - MINIO_ENDPOINT=${MINIO_ENDPOINT}
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY}
      - MINIO_BUCKET_NAME=${MINIO_BUCKET_NAME}
      - MINIO_SECURE=${MINIO_SECURE:-false}
      - MINIO_OUTPUT_BUCKET=${MINIO_OUTPUT_BUCKET}
      - UPLOAD_BUCKET=${UPLOAD_BUCKET}

      # GPU 配置（根据实际 GPU 调整）
      # 这里api服务要使用gpu是因为实时的office文档解析
      # 需要调用一些小规模的模型做切分以及NER,占用的显存有限
      - INFERENCE_DEVICES=${INFERENCE_DEVICES:-0}

    volumes:
      - ./logs:/app/logs
      # 挂载源码目录，实现代码修改后的热重载
      - ./src:/app/src
      # 注意，这个很重要，因为有不同的服务都要访问这个数据库文件
      # 所以，这里要挂载到宿主机的目录，而不是容器内的目录
      # 否则每个容器都会在自己的目录下创建一个数据库文件
      - ./database:/app/database
      - ${HOME}/nltk_data:/root/nltk_data
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface

    # 其实这里原本不用GPU的,但是为了切分的逻辑，所以要加上
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    depends_on:
      # 一切组件都启动之后才可以启动网关服务
      # 否则背后实际的操作都会失败
      redis:
        condition: service_healthy
      jumo-worker:
        condition: service_healthy

    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:${API_SERVICE_PORT:-5116}/docs > /dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

    # 直接以前台方式运行 API 服务（而不是 nohup 后台）
    command: >
      bash -lc "$PYTHON_PATH src/jumo_service.py"

  # Celery Worker 容器
  jumo-worker:
    image: jumo-service:latest
    container_name: jumo-worker
    restart: unless-stopped

    environment:
      # 模型与下载源配置
      - MODEL=${MODEL:-opendatalab/MinerU2.5-2509-1.2B}
      - PYTHON_PATH=${PYTHON_PATH}
      - PYTHONPATH=/app/src
      - HF_ENDPOINT=${HF_ENDPOINT:-https://hf-mirror.com}
      - MINERU_MODEL_SOURCE=${MINERU_MODEL_SOURCE:-modelscope}

      # Redis 配置（通过服务名 redis 访问）
      - REDIS_HOST=redis
      - REDIS_PORT=${REDIS_PORT:-6379}
      - REDIS_DB=${REDIS_DB:-0}
      - REDIS_PASSWORD=${REDIS_PASSWORD:-}

      # Celery 队列名
      - WORKER_QUEUE_NAME=${WORKER_QUEUE_NAME:-celery}
      - VLLM_BASE_ENDPOINT=${VLLM_BASE_ENDPOINT:-vllm}
      - TASK_NAME_PROCESS_PDF=${TASK_NAME_PROCESS_PDF:-process_pdf}

      # MinIO 配置
      - MINIO_ENDPOINT=${MINIO_ENDPOINT}
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY}
      - MINIO_BUCKET_NAME=${MINIO_BUCKET_NAME}
      - MINIO_SECURE=${MINIO_SECURE:-false}
      - MINIO_OUTPUT_BUCKET=${MINIO_OUTPUT_BUCKET}
      - UPLOAD_BUCKET=${UPLOAD_BUCKET}

      # GPU 配置（根据实际 GPU 调整）
      - INFERENCE_DEVICES=${INFERENCE_DEVICES:-0}

    volumes:
      # 挂载日志目录，实现日志持久化
      - ./logs:/app/logs
      # 挂载源码目录，实现代码修改后的热重载
      - ./src:/app/src
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
      - ${HOME}/.cache/modelscope:/root/.cache/modelscope
      - ${HOME}/.cache/vllm:/root/.cache/vllm
      - ${HOME}/nltk_data:/root/nltk_data
      # 注意，这个很重要，因为有不同的服务都要访问这个数据库文件
      # 所以，这里要挂载到宿主机的目录，而不是容器内的目录
      # 否则每个容器都会在自己的目录下创建一个数据库文件
      - ./database:/app/database

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    depends_on:
      # 所有依赖redis的服务都要在健康检查通过之后启动
      # 否则背后实际的操作都会失败
      redis:
        condition: service_healthy
      vllm:
        condition: service_healthy

    healthcheck:
      # 注意：当前代码里 celery 配置关闭了远程控制（worker_enable_remote_control=False），
      # 所以 `celery inspect ping` 会一直返回 "No nodes replied"，不适合作为健康检查。
      # 这里改为检查 worker 进程是否存在。
      test: ["CMD-SHELL", "pgrep -f 'celery.*-A src.celery_worker.pdf_process_worker.* worker' >/dev/null || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s

    # 使用与宿主机相同的主程序入口，在容器中以前台方式运行
    command: >
      bash -lc "$PYTHON_PATH src/celery_worker/pdf_process_worker.py >> /app/logs/jumo-worker.log"

  # vLLM 推理后端容器
  vllm:
    image: jumo-service:latest
    container_name: jumo-vllm
    restart: unless-stopped

    environment:
      - MODEL=${MODEL:-opendatalab/MinerU2.5-2509-1.2B}
      - PYTHON_PATH=${PYTHON_PATH}
      - PYTHONPATH=/app/src
      - HF_ENDPOINT=${HF_ENDPOINT:-https://hf-mirror.com}
      - MINERU_MODEL_SOURCE=${MINERU_MODEL_SOURCE:-modelscope}
      - INFERENCE_DEVICES=${INFERENCE_DEVICES:-0}
      - VLLM_RPC_TIMEOUT=${VLLM_RPC_TIMEOUT:-120000}
      - REDIS_PASSWORD=${REDIS_PASSWORD:-}
      # 用于健康检查的端口列表，逗号分隔，默认只检查 8000

    volumes:
      - ./logs:/app/logs
      # 挂载源码目录，确保容器能访问到启动脚本和代码
      - ./src:/app/src
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
      - ${HOME}/.cache/modelscope:/root/.cache/modelscope
      - ${HOME}/.cache/vllm:/root/.cache/vllm

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    # vLLM 对多 GPU 会在容器内启动多个进程
    ipc: host

    healthcheck:
      test:
        - CMD-SHELL
        - >
          if [ -z "$${INFERENCE_DEVICES}" ]; then echo "no INFERENCE_DEVICES"; exit 1; fi;
          i=0;
          for g in $$(echo "$${INFERENCE_DEVICES}" | tr ',' ' '); do
            port=$$((8000 + i));
            curl -fsS "http://localhost:$$port/v1/models" >/dev/null || exit 1;
            i=$$((i+1));
          done
      interval: 20s
      timeout: 5s
      retries: 5
      start_period: 90s

    # 启动 vLLM 后端：由 Python launcher 负责按 INFERENCE_DEVICES 拉起多个实例并监督进程
    command: >
      bash -lc "$PYTHON_PATH -u src/celery_worker/vllm_backend_start.py"

  # Redis 容器，其他服务通过服务名 redis 访问
  redis:
    image: redis:alpine
    container_name: jumo-redis
    restart: unless-stopped
    environment:
      # 设置redis实例的密码，保证前面的服务可以正常访问这个redis
      - REDIS_PASSWORD=${REDIS_PASSWORD:-}
      - REDIS_PORT=${REDIS_PORT:-6379}
    command: ["redis-server", "--requirepass", "${REDIS_PASSWORD}", "--port", "${REDIS_PORT:-6379}"]
    volumes:
      # 对于使用来说这里没有什么实际意义
      # 这个只是为了保证数据的持久化
      # 效果就是容器重启后，数据不会丢失
      - redis_data:/data
    # 配置健康检查，确保redis实例正常运行
    # 所有依赖redis的服务都要在健康检查通过之后启动
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -p ${REDIS_PORT:-6379} -a ${REDIS_PASSWORD} ping | grep PONG >/dev/null 2>&1"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 5s

volumes:
  redis_data: